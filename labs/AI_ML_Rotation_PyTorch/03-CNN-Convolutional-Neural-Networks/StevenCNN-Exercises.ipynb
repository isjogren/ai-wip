{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Exercises\n",
    "For these exercises we'll work with the <a href='https://www.kaggle.com/zalando-research/fashionmnist'>Fashion-MNIST</a> dataset, also available through <a href='https://pytorch.org/docs/stable/torchvision/index.html'><tt><strong>torchvision</strong></tt></a>. Like MNIST, this dataset consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes:\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "<div class=\"alert alert-danger\" style=\"margin: 10px\"><strong>IMPORTANT NOTE!</strong> Make sure you don't run the cells directly above the example output shown, <br>otherwise you will end up writing over the example output!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform standard imports, load the Fashion-MNIST dataset\n",
    "Run the cell below to load the libraries needed for this exercise and the Fashion-MNIST dataset.<br>\n",
    "PyTorch makes the Fashion-MNIST dataset available through <a href='https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist'><tt><strong>torchvision</strong></tt></a>. The first time it's called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='../Data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='../Data', train=False, download=True, transform=transform)\n",
    "\n",
    "class_names = ['T-shirt','Trouser','Sweater','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create data loaders\n",
    "Use DataLoader to create a <tt>train_loader</tt> and a <tt>test_loader</tt>. Batch sizes should be 10 for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ../Data\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: ../Data\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine a batch of images\n",
    "Use DataLoader, <tt>make_grid</tt> and matplotlib to display the first batch of 10 images.<br>\n",
    "OPTIONAL: display the labels as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  [   7    5    9    2    2    0    0    1    6    4]\n",
      "Class:   Sneaker Sandal Boot Sweater Sweater T-shirt T-shirt Trouser Shirt Coat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAABeCAYAAADhaVpbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXmQnFXVh5/bPUt2kpiELOwBFcIaVmUJAgoCFqDIooSgHyAggpaIgPCJWhbBEimBUgkQwRXZFFTkY5UdDCBLCEnYAoRAdiCTZDKZmff7o+d33zu3uyfTM2/3JHCeqlRner3be997fvecc12SJBiGYRiGYRi9J9fXBTAMwzAMw/iwYAsrwzAMwzCMjLCFlWEYhmEYRkbYwsowDMMwDCMjbGFlGIZhGIaREbawMgzDMAzDyAhbWBmGYRiGYWRErxZWzrlDnHNznHOvOOfOy6pQhmEYhmEYGyKupwlCnXN5YC7wWWA+MAM4PkmSWdkVzzAMwzAMY8OhN4rVHsArSZK8liRJC3AjcEQ2xTIMwzAMw9jwqOvFZ8cBbwV/zwf27OoDzjk7P8cwDMMwjA2FJUmSjKzkA71ZWLkSzxUtnJxzpwKn9uJ3DMMwDMMw+oI3Kv1AbxZW84FNg783ARbEb0qSZBowDUyxMgzDMAzjw01vFlYzgG2cc1sCbwPHAV/JpFSGsYEzatQoTj21INRuu+22AMycOROAd999F4A1a9YA0NzcDMAee+xBY2MjAFdddRUAr776asnvz+VytLe3V6n0xuabbw7AG29UbKx6DjzwQADuu+++TMpUDa666ipaWloA2GqrrQBYuXIlABtvvDEABx10UNnPO9d546KnwVBZcvnllwMwZMgQAP72t78B8PjjjwPQ3t7OFltsAcABBxwAwCc+8QkATjnllFoW1eOc63bbjRkzhk996lMA3Hbbbf7zUJgXANra2jp95uSTT/avTZs2LZMyVwNdd+qHfv36AVBfX8+bb74JwEYbbQRAU1MTAKtWrQLSemk89+Uc2eOFVZIkrc65M4H/A/LA9CRJXsysZIZhGIZhGBsYPU630KMfs61A40PK5z73OQDOOussAOrq6ry11b9/fwBGjBgBwIIFhR3z1157DYBDDjkEgHw+71+TqtXa2grAjBkzAPjpT38KFKwyWaC1tspGjx4NwOGHHw6kStwTTzxR9N5hw4YBMG7cuE5/z5pVyMqydOnSos/U1RXsPdW92my22WYAnHDCCXz/+98HYNGiRQBcf/31AFx33XUALF68GChY0FBQBlSnKVOmAHDOOecAsGTJEgCGDh0KwLXXXuvVq4ceeqhq9dG4gPJj46KLLgLg9NNPZ86cOQC88847QNr+O+ywA5Cqpl/72teAtA26+u2+VFN1T3vvvfeAtP2leIwdO9bX8YMPPgBSdStW4GpFKcVKZdlvv/0A+MIXvgDAzjvv7MfcvHnzAPjYxz4GpHV9++23ARg+fDhQmFuk5GhuufvuuwGYPn06ULvrrSv+9a9/ATB+/HggVd5yuZyfP8PxDdDQ0ADAT37yEwCmTp0KFMZxRnV6OkmS3Sr5gC2sDKMCdENdu3YtkN5MtaB66623/OtaUGlCE4MHDwbSG5i2BD/44AO/DaPJY8CAAUA66Wmr8MQTT/RSeDUZPHgwF1xwAQBHH300kE7WelTZ6urq/KStcuoGK/le71XZly5dysUXXwzAjTfe2Om3dWOp1hz15JNPAun2Qy6X8+VSubfccksg3W7QYln1u/rqqznxxBOBtD81BtQWgwYN8p9R37/wwgsAfPazn82sPqUWNZ///OcBOOOMM4B0Ea8b77Bhw/znHnnkEQD23LMQ3B2PRfXd4MGD/Q3w2muvBeCBBx7oVJZq910p9t13XyBdtKqOumZ1Pa5du9Zfc7o2NQZ23313AJ566qkalbpAuLC68MILgc4GF6T1Wbx4sV9Aqa9lCKivdL2NHTsWKCw+ZACpHWTsCNVZv19t8vm8Xzjpurr//vuBdNx88pOfBApjT3VdvXo1kM6but7mz58PwNe//vWsi1rxwsqOtDEMwzAMw8gIU6w+QpSzInfccUf23ntvAH7961/XvFwbMvfccw+QKljLli0DCtaY5PoVK1YAqQIg1ULbELLEcrmct8KkiKivpJjIsn7++ef5wQ9+kFk9YiVur732AuDOO+/02yRSLFQPWY6ygIcMGeJVDyH1I7Yyw21S/fZLL70E4B1zpR7FZestN9xwAwBHHXUUkCoBuVyuyPlXf2urRc+rP5xzDBw4EEi3yKRUSVUIFR/VW214xBGFnMrahsuKRx99FEjbWeVVm77//vtAwWl7zJgxQOqsru0lqaXantaW4NChQ70Kp7rLMVwBG6ISp+zeIodnOTFLxdH4CbeFNBeqH3RdnXzyyUC69VtLpHyqDM8//zyQjnttebW1tfm+2WSTTYB0DGt8jhxZSLu06aaFwP333nvPq6Qaj/peXb9Sh+69914uueSS7CvYQan7kJS1Bx98EEivJZV1yZIl/nO6VvS3lDcFAR155JFZF9kUK8MwDMMwjL6iN+kWjCpTypGyJ9afLJ3Y10d70QceeCD7778/AK+//joAd911F5Du78fhux91ZN2NGjUKSK1jqTtr1qzxzrNqf1n3UrBkSatfVq9e7Z8Tcd/JKttuu+0yqUdsvYpf/vKXQEE5k2Ihy1afkRqiMdnU1MSkSZOAVB2QUhL7Uuj32travGqgOt1yyy1A6mOSlSO0+krpA6Quqs3D8GwphuorWcly3Nd3rVq1yreP1CAplfH129DQUBTQcNJJJwH0SiGIfauuueYar1SEKQbCeql/Zs+ezbPPPgukaUFeeeUVIFXc1L/qs+XLl3vHfI3pyZMnA+l1UCs/nRClTBCxX2Lo9Kz/x30kpacvkHqpaz1MNQBpffL5vB+7Gocal7qWVA+Nxaefftp/j+YU9afUL6lee+65Z1V95Ep956GHHgqkdbzpppuANIBi4sSJLF++HEjnlPj79Fn5LWo3oS8wxcowDMMwDCMjTLFaj+mNtZDP571lIv8W8ZWvFPK4ShGYN28eN998MwDf+ta3gFSxMqWqNLKO48R8CgleuHCh7z/5FUk1kAqiqDp9x5o1a/xzUkhkbUsNkU9F//79yyqRlaDvkBJ28MEHA7DLLrv4ckiViJWROOy5tbXV+0YoQk7vlfIZt1d9fb33OZNvksLLd955ZwCvqDQ0NPSqrlJT5Mvyxz/+EejsL6U+07hX++sakv9ReG3GKlQc4h22hZSEa665BuidUiViRe+ggw7y/ntSn1RG1UvqxYoVK4oS1Sopqt6jz+oR0v7UeJRS8pnPfKZTWWrpwysfMaF2icdra2trkQonpPD0BVLBlYJE10rsY7VixQo/LjUe5ROpNpByLpVnwYIFXtXSGFC/a0xKBWtpafHJbe+9995sKxnwv//7v96fcuuttwbSa+XTn/40kNZ99erV3s9R85HaR59R0tdzzz0XKKRP+fe//w3Ab3/726rVoxSmWBmGYRiGYWSEKVYbAM45b23Fln9sEcqib2trK1KqfvWrXwFpBIYUgiFDhviIHx1rodxCesyackdhhPWSz42sFykbsQoihaC1tdU/J4tdviBqC33HnDlzmD17do/LL3+UWEGRb8uqVau8eiNFRJaVVCkRWtYqZ+iDBPDxj3+8U52zipCL1Q7lPVI/NDQ0FEXK6bVS6pO+T+0iFUfWsfo59N2L+16WuXxOpFjpM71FSToVHagcT4sXL/Z1iZUrlU1KgQivTdUxVqykBG255Zb85z//AeC73/1uJnUJkZ/K2LFj/fWsckvJUBk1FocNG+b/r0i/OApUbSFfnJUrV/rxJ6VKY1x5v6REyMerFuhaF1I21If6u66uzl9nqqtQfWrNJpts4ucFqVCap9WXoe+V+kbqt/yjVEehRLz5fL5IYY7vKVK3JkyY0ClCMGs0n0+ePJmFCxd2qofGqa6Z8HqU4qyxp2tR+as09vSdm222GcceeyyQJh7VPa/amGJlGIZhGIaREaZYbQAkSeKtrkp8nhRxpINGY8tEfy9btsz7Ftx5551AejxAtRSrcr4XoQp1/PHHA7DPPvsAnY8RgdQ6k9VZX19fpPjIGo+t11dffdUfQ9MTtt9+eyBVwmQJ7bHHHkDB2pTlpLZVu8tfKj5GI5/Pe8tf5ZdfhHy6ZM3W19d73whZqz0hVtxkqYZ+YbGPh8qrtlRftre3+//LmtRnY/+sMCIy9oFRmeTv9aMf/QgotHVvopXiMuj4GvlkrFixwlvOQuMxVknD39dz8kGKfZak9IwePboo23WWnH766f73NTbkp6Yxp/YL6yNVUbmthK6ZeIwMHDjQKzuqm8aG+vWrX/0qUFvFSkfWqB5SsJSjSr5L9fX13hdSkWPyz6nFaQalOOuss/y1rmtGipWubz0fqsxxtn/VUcq8+nvgwIF+XMaRx/pbYzOfz7PNNttkVreYs88+Gyiog7qONMY0V0rt13za3Nzs++i5554D0utMR1Jpfg0jnTUuTzvtNKB697MYU6wMwzAMwzAy4iOhWPXv37/I36gctcwUXAnHHHMMkK7SlW9KFpYssF133RUoqDzyl5LlHOYbgnSFP3z4cP+crCJ9/1//+lcgzVJdbcK2V58pskkWqCwSWTfhIcfyrZIlJzVBlovqudtuu3k/KWX8rgRZSSqLHuWnMnjwYK9IyYqMcwnFuXQGDx7sfSdkRcpylO+WDszdaqutvE9GTxSrcsqPonM0DvL5fCdFKiT22Qi/K/YBi/2yQl+sOLeSXlNbdqfc3aFcPqwTTjgBKOTu0vfHVr3GUeyDFfpYaczpb401qTqKQuxtPcohhW/JkiVePZOVr0g/zROh35/KqfZWPeL5IswLpTlFUYFx3i9FHNcSRdLGEXG69nXocENDg1fDlatL19fcuXNrV+CAyy67zJ91qHZWHjzNH6pXW1ubb//Y51Kqk9RkvW/t2rUlI0JDNFba29v5xS9+kWX1OqHzGBcsWOBV0TDiEVLFSjsDM2fO9HWVH5auIbWLVFTdA0aOHOnbRffFWmGKlWEYhmEYRkZskIpV7CsRW3/x66XUqosuugiAn/3sZ0Bx1FIp4rwZ+v6scz3F2c6HDh3qMxvLmlF5teKPc+msXbvWnzWlU8sVVTF+/HggtdTnzp3rLU5FVuizcW6YahOqClKFZGWrXWRlqj/UJv369fP11/dILYgjz5xzXg3qiWIli0qqmax9WcnOuSLfJI0bKQB6DHMm6TOxb1IpVUQn2PcE9bcsWrVpnMG5f//+Pr9N7B8Vt2l7e3uRiiViH6Uw6ic+H1Gv6Qw7+QstXrzY92+W0VtSkiZPnsxOO+0EpFZvmLupVD2cc0XP6W+VVeU/66yz/PdkqVhJbVE/hRn8pSipTOpnKQN1dXV+DonzPgn566hfNtpoo6LvjxWScuOgmuh8RLVt7BumeoXZ1eV/pX5+7LHHql7OUixcuNDnNNM4kX+RVKj//ve/QKHvVLf4ehJxJG9jY6NXKdUOmmN23HHHTt8xZcoUr3BmiXKc6dp1zvl66FFzue5NKkdjY6P3nZswYQKQKnp6Pv6ugQMH+veoDeW3ptMSqsUGubAqFZofooGjrbDzzjvPS4qSVRW+rovx/vvvBwoTTRwuHf9uVqHu5YhTEUycONFP9BpEYegwpDf21157DSgcRBofiBujxcH06dP9lp8Gni7YLA/6rRQtiDVJaGKInZx1Ax46dKh/TTdpPWqbQDe7WbNm+UVQT5D8HN+AFRo9atQov6hTPeLFgMaTblJr1qwpcrrXezTpaWw3NTUVhYpXQjwRX3rppSV/N3SEVtn02fi9SZIUGTtxXeO/BwwY0CmMX78J6cHHSuh52WWXVTUc/vXXX/eJUeNFcVhH6GzU6f9qjzjpqtpCk3zWyPlex5u88847/tpQmeLjh1TmsM/CBJqQXl9awKte4aJe3x/PmbqRXXLJJZx//vkZ1bRr5MAtYmd8zQHhFnNcVy1O+4I77rgDwB+W/Je//AVIy6YF0IwZM/x1EKdMiLcIQzTP6N4xceJEID1w+oc//GGn38sapVkI79saP3F9hO7XuVzO943GsPpR9wU9r3Ha2trq66zv16Hwut9VC9sKNAzDMAzDyIj1XrEqpUqtSz7/zne+A6SK1bHHHuulb1l1UhHkMCjFqpxaBemK//rrrwfg29/+NpAqP1kRlyE8JkJbTyqLZPq///3vABU5Hf7+978HCoqA1A9tT2l7Qdtx2hrMmnjbNuxbWSuy1KSmxYqVLJQ1a9YUWS9xYke131tvvcXDDz/c43LHzsxSBEKlJk49ICVGlpYsavV3Q0ODr4teixUGqTuNjY2+73uDHJ611az2k7IXWpUqf2xVVnJYuPosTJ0xc+ZMID1wNT5y45vf/CZQcC4PD6IN35sFjY2NnVJfhPWIH0sR9iMUJzwNKdVmPeXHP/4xkPbZvvvu67f7Nc+VUxBDxSoO+IjbNtwiVP/FSW81FyptS62PEoFURY7HqdTkcePGFfVVLdNClCIMmlLgkLYChRSl3XffvejYGxGn+BC6liDdCv3DH/4AwJ///Oeisogsgyu0SxQGOsRzY+wuobE4cOBAH6Sj92h86m/NXXrcZZddOqWngXR3o9qYYmUYhmEYhpER64ViFYZ0a4VazikvRFaZQmeVRFGpCZTqfunSpf57pBZopfzFL34RwFvNt956a9nfmzJlCpAe16AQTh1Y3FvK+YzttNNOPtmkyi8LUUpSrFTl8/luW/MDBgwoOhhYVo0cJrMiPpInVuf0/F577eUTg8o3RX4vcqiXhR4mAVW5ZbXqs9pjl+9HT8OqpezJ+pKzqxQrWUjDhw/35VUdY1UwTJKpMuo5WZzql1idW716dZFV2hN0tEp8vcnJGVJrUmVRXePUA90h7v/ly5d7dS72C1GdFVBx5ZVX+iSY1TgcvL29vShAZV11zOVyvixxoEGt0rZIJTr55JP9czrIWn40CtLRPCLCtBLlgnPi11tbW/1uwKmnngqkTsY6sqcv0TUZp3zQtbnrrrv6umg+7Y16nQVJkhTN//HfL774IgB77713UZqFeC7pCs2F8fEu1UgBEqL5Wr6tAwYMKArAidcBmo9aWlqKfDHjumo3IvTVVP/Gh9hXG1OsDMMwDMMwMmK9UKxC6zMOBRWyoqZOneoVAB1JoUNa5Rek6I9wL1ZW3csvvwykicfkv3PLLbcAhRWz0hPoUSqBEkrqexUSmpViFR8qrLDS5uZm/1qcOuI3v/lNp79lFaxevbpsxI4iIxTeO3bsWH8chNB+drnkk6USqcb7/WFqg3WlpjjuuOOANBHpmjVrfL9KmVQqAH1vnLhw7dq1RVEysmKkXMlKk8VdKUrREB8zIX8nRa6Eik/sPyYFSGWTArRs2TJveaqOsr70WfXp+++/3ysfK6lnsSoYR8G1tLT496oe8mHQe8sl3uwK9dnQoUN9X8gCDZOThs9XO+lkkiRFaoEo5xNVyh8lVrtKRRFn6WMV+ykCPPTQQ0CqOF9++eWd3hOqa7omYzUxLqtU/tWrV/sxcfPNN3dZtr5IuCx/WY0X1VnqR2Njow+3V1+pvfqScpGzIrwOY/+xeKcnjs7N5XJFSW91ryj3e1mj6zico6UoaXdJc6KSQWuOy+Vyvi7yk47nrPiA9FLH+PRkruoJplgZhmEYhmFkxHqhWI0dO5bDDz8cSH1L5EcjHwGtVrfeemu/6lRiR+3TKgGm0Gq4vr7eH0YqRUEHOeozWgUPGjTIKyRSJ+SvoxW3fE0UpVYttMpubm721lacl0PqgQgVrVipUoSODkjVXvfLL7/sFb1QEamU+NiSUuqUrAspVF/60pc61UuKUlNTk8+3Iv+N+EiGODIy9NeRr5jUID0vK61UnpfuEEdbyUrSmJTKedhhh/nxEluCsa+A6tW/f/8i/yu9FkeYJUnikxv2BPkSqvyy/vS7YaJZKS5x/8aEfiLlKBXFFOfqin2V1Cabb7659+mZNm3auitZIQ0NDWWj57qqc6n/h5+phj9YSGyFh/6Vug7iKLLQn2pdilV8zFL4eqyKdxXlWyvkL6t8ULoPyL+yX79+fr7XvHfffffVupgVox2Z1tbWomS9MfGYKzXfaWzUCo0b5aebN28eTz75JJAe/q56aZ7THDl8+HBfB/WZ+lXP694h/9cXX3zRv0ffl4VfancwxcowDMMwDCMj+lSx0n7qUUcd5SP6pFDJe1+rXFnQTU1NZSMHhKymMIeL9lq1cpVviVa58pkZOXKkt+LjqK7YMuxN5utSxFaGfIwWLVrkLaz4PfFBtWFkh9pH2eW32247IPW3kI/ayJEj+dOf/gSk6p98zsoRWqLxwbox48eP58gjjwQKES2QWrrydSiVUV7tG++Pqy/Vv/JnGjt2rLfCVD4pelJANTY03ipFUYeyfDQ24lxPjY2N3tqShaayxDmwQh8xvUftIH/BWFVob28v8imoBEVOyd9O36syaxyEhwzHlFJ31qVYxWNko402KopGUn+rLTTGV65c6X0iq8Ho0aN9O8fH1JSjVH27o9zWCiny8TFEpfo0VqxKZdbX87NnzwbKR/X2JTNmzADS+4vuJZpPVq1aVXR8Uq18b3qD5p4wsjPOOdbVXBw/p3tKrVBZpZ6uWLHCR2rqxAOdHKL+0W7EqlWrfF9pB0njU/XQ+NSu1Ny5c/38pvVDmM+rmphiZRiGYRiGkRF9qlhpFfn44497n4k4V5FyRulx0qRJfg9VPkKyTMqt3sP/6zWtbmXNyF+qra3NvxbnpxEqo3xohg8fnskZU7ESp9+fPn26j/575ZVXOn1G+WpmzZrV6TsA/vnPfwJpNJ2yqMdnli1evNhbDsr2u668LqEfR+yHcthhhwGpOjVu3Dj/XmUVltUV+0KprQcNGuTVRT3qvVKq1E7hwcvxQbLlItp6moFXvymrSf4/+lvWWEtLi6+zrHo9hgccQ+dM41K3Yh+r2P9o7dq1XZ4SsC50vSmPlX5HVrHGdhiBlEWepjhrfi6XK7qO1UelDtM988wzK/7N7jJ27NiiaKpyB7yLMOotztau99ZaxQl/L868Htevra2tKLozjjAr1e/rgzJVDqnfelT9NOeEUZrVOGy4t5TLJ6V5fNmyZV6t0eO68le1t7f7OUVzVa19rDR+wmhmPSeFXq9p3tbckCSJf4/mCc2Jeo9eVzs999xz/v4e71hVm3UqVs65TZ1zDzjnXnLOveicO7vj+eHOuXuccy93PA6rfnENwzAMwzDWX7qjWLUC302S5Bnn3GDgaefcPcBJwH1Jkkx1zp0HnAd8v5Ifl39NLpdjt912A9LVqHyf5Aty++23A4WVq6xqRUUpM3P8t1Svurq6Iisszvch67+trc1b71IWZPXFGV/j3EVZEftkzJo1yytSI0aM6FQGnaP2wAMPADBnzhwALr30Uu+PEucqEmFET3xunXx7ulNGnWWoDPhC7bZ48eKi6Dnti8d76bKiWltbffnk7xNnWo8tlzC3k35HfRbmsIGe77WH6hikkUZS/FTWlStX+jEWn22lOsbRdv369fMRp7FKIEI/CX1vT5BiJV8x9X+sEg0YMMCPBZVXPm2l/GviaLBy/llqx8WLF3PssccC6XltsY+j+mrcuHGcc845APz85z+vuM5hOcMyio033tj/1roUmVJqXfwZ1b1c5FYtiecUWe5tbW1F0X5xHq5yGdjXVzR+dC2qfooSPOigg/x7db2tj6iddb1JxVm6dGmRolruOhO5XM5f02ofzV2lfrca0ZxhbjwoXN+6R2jMxdnZw3uUxrBe0/yqdtE8FZ4dqzkkPlew2qzzV5IkeQd4p+P/K5xzLwHjgCOA/TvedgPwbypcWIn58+f7BGFqFC2O5MwehoWrM7RloMWXDv4U4aCLD7ONJ9dSztjxFld84O66BnOp74qfh+4dR6ADM3XgquqhQXbFFVcAcMMNNwBw9NFHM3XqVCB1CIzLEt4Y4/QK3TkaQVt9F154IZAeuSAkzba2tvrJIQ6t10IxXrDk83l/EcQTv8aIXtf22aBBg4oSa4aHCXe3Xl0RLwSFEjHK0XrYsGH+otajJGotcDWO1U4tLS2+nPGhzroB6O9ly5Z1a/yV49BDD+30veW2FOrr6/3EpfeWO3IjpFySyThR7KhRozjwwAM7fU88PsNgFG1d9mZhpe+LFxsjRozgrbfeqvj71pVUVDeA+vp6PxZqnYYg3rbXNdTc3Oz/Hzu4a9EdL5bX961AXYOac/S3tv1CQ1jG9/qI2lv1CLdsyxleotSiOA4CkmFa7aNsROy64ZwrCh6I0weFc0G8Va35Qf2peSpMraD7o+ZcbYNWm4pmZufcFsAuwJPAxh2LLi2+ahtiYBiGYRiGsZ7RbV3MOTcIuBX4dpIkH3TXYnHOnQqcWuY1//9YUdCWlh6lFPTr16/IKVSSr5zaZWnpu8JDnrXqjRNFhg7veo8ckUUsiXdHRi5nBYTPr0tFyefzTJo0CUiVBa3spXpoC0MHsb7xxhtexYp/s1SYvFbyUnrWxaRJkzjjjDOA9PiBLbbYAih2gm1ubi5q91jBkiUfWhtxn8lCiY9GUN0XLFhQdLxB/Lv6zIQJE3z6CW2zdgd9Xu2k31P7aTtalhIUJ1GU5SgLS99ZV1dXFGShuj3xxBMAndSdnoSI6ygnjZ84VUnc5m1tbUVbWllYtuHvTJ48GUivJ12/Uu3UBk1NTWWdynuDFMP58+f7PorTLJSb79rb24vcC+I21BiZMGGCT6HSV4qVCMek+jV2j9A1pLEejtPeBE5UG4XuS6mKr5Nwvq21A3d3iMeGjjaTe0x4WHichkXo9VKHbOu96l/NxZqzwmOOskTjR3NOkiR+DtS1rjlAuxDaSWlubva7BOo/vSY1Si5CCihqaWnxvyllMj7Gp1p0S7FyztVTWFT9MUmS2zqeXuicG9Px+hhgUanPJkkyLUmS3ZIk2S2LAhuGYRiGYayvrFOxcoVl7nXAS0mS/CJ46Q5gCjC14/H2Sn+8EqtNVmspp+OsncdrhSyF888/H0idKmVRyHLfZpt3rFJyAAAORElEQVRtuO22wnpWFq+ssDghplIqfOMb3/C/I4s5VsZCJz8pGN31Qdp+++29JRWnP4j3yaH4eBJZKKLUAbaxVRwfICulLE5rEH5frHZp/Dz55JPeKqqE2L9OFqHGoA5+fffddzv5TkGqysli03ep/fr37++tSFmPKmOsJA4fPrxH/gJXXnklkFqr8UHWsY9XLpfz5YuToZa6fsv5EsbO7BorCxcu9Cqfvjd2ONV3rFmzxjvcXnTRRUDqc9gbpHTX19f78dHTI4+g2O9Fvj277rqrv35r7aNUSWLQuI/iPszn80VK5/qE/HNFPB9pjoTskzxXg7g+SZIU+Z2WC5wQYaLf8AgtSHcaNOdUK1mq5ppQDYsVN6U9iv0tnXP+vaF/JqTKZHxYeJIkRYFovbmuK6E7W4F7A5OBF5xzz3Y8dwGFBdVNzrn/Ad4EvlydIhqGYRiGYWwYdCcq8BGgnHl1YLbF+fAQh9hr5a3kmfvtt59XAmShP/PMM0CqJGmFf8UVV/j94zhEVlaIrDDtOytKDYpVqFJKQ6WHpt5+++3+YGupZlLg5OMQphOILatyPjKhpRInao1DZvWostfV1XVKoBn+jt6jBKu77LJLJz+o7iIVSn2nSL8333wT6JwqQKpPHEosJVKKU+gbEPqxQGqNzZ07F0iVpMbGxh75C1x22WVAOm6U5iQOYQ79beJUFV1FuK7LIoz9/AYPHuzVPqlzccLN0Cfk5ZdfBoqPcuoNUl5zuVzZiOBKiH1A9Thx4kSuu+46oO+OuYlVi1wu59WPOPFrGM0I6TVUX1+/Xh8Bo3JLfYyV73CMrg+pMES5OXGrrbYq+97uPh/6b+p+oPQEUsR0z6iW/18cFdjY2Oj7SP5jmmtKKU16rz6v8mtO1twon6vwetb3rZdRgYZhGIZhGEZ5+vRImw8z5aJmdGRMU1NTUWSicujE1vL48eO9hS6rUqqHHqWgHHzwwf631hU5Fe7DSx3rroU+f/58TjnllE7PqR7KPRbmm4pzysTKhhSaMEmh2lA+ElKh4sStYQSMrJf4WJQ4IqalpYWZM2d2q64hsvZiHwcd/CofgfCoDOWLkcUVR7+F/ipxO8R5n/T6smXLfP0r4R//+Een8t96661A2v/hIdLQ2eqLo956Qlyv8LtiNUV11TgaMmQIV199NQAXXHBBr8sgtt56a6CzErOuRIJdHb4cK636zp122qnHZe4tsQoVRu525UsFxQlD6+vrvV9aud/pS1599VWgODJYKFEobBj+uZo7w/k69qGK/R7jfi51vFs8LuXHWy3UH+F8rufiiG9FAErFXr58eVECW/lYabdA15t2b5qbm4tUds3F1cYUK8MwDMMwjIwwxapKHHDAAQBMmTIFKD5UEtL9Xq24pRbEB1GGubWkckmZ0er8tNNO6/T7DQ0NFWUZj6PDeoIsBqk3H0binFRxzjOpanV1dUXRmFKJZIGG6pleVxuqfzUGFi0qZDORhd3bg78vvvhigKIjJUR4mHWl/nfdoZQPU3jwMxTntlmxYgXnnnsu0DvFKq6HDmotVb/Y7yjO0VbqxIbYsla99Dsh1ch63ZUPZRx52dDQUOQTFkdq6fUwx1dP1NJaoZxvcfSbCBUrHUW1PhMfHJzP54sU01jpFqUOTI/9ymoVGamySTVaunSp333QPU/3Ic1t+nvEiBFe6dJReHEeLl1n+kw+n/e/pdNHwgO4q4kpVoZhGIZhGBlhilWVkDIlvyllfpUV2NDQ4POHfPnLhUwVUrAU1SBLq62tza/gtUqXH5M+qwNsZd1UujKX75OixIzSqH1LnTEJaf+OHz/e93144C2kFpasNVmMLS0tZU8GkJUmJeu9997zY6EnyEKMz66L/S/i/4fv6YnKUiqisFzOsVgdWrt2rVfuenOYdhyRp8jOlpaWIpWm3Jmioe9K3D5xNJ186JYuXeotaM0PtTqnrVz/hqdSlFI39B5IFYEkScoqVuuDj5XmynJlCZX8DcHHKo7KDP2rykVVl8tvBcXjv7fnp3YXXQdSj1auXOnPCNZrmo/i0xdyuZyvt5RfvVfPhwfHA9x1113e17XcfF0tTLEyDMMwDMPICFOsqoRygoT5pMqhFfi2224LUJSzaosttvBZxp977jkA7r//fiBVt4RW7d2J7gstbeW/mj179jo/91FGSomsIuXFEvKp22effbwlJUUktsbibPPvv/9+UVSjFI5HHnkESBWahoaGHmW/VgSQzteUahZHfoXWYBzBmUVUYOzPA6lSpeekiuj3Vq1axYgRIwDYYYcdAHjqqad6XBah6y30cQtzNoVliiMWnXO+3LHSJjRmRo8ezX777QcU8sCF398T5a0cpfonrpcIfeji3Hsizsje3t6+Xs8TccScFBKhawyKz3utlYIY09V8rTGiOaG9vb1THrLufE8ul/P9HCqP0Du/2kqQL7F2W95++21/35GvsMoW+3yG2eZL5dqDtB663t544w3/fYou1P2z2tjCaj1Acqges6A7E0M4md91112dHo3SaKGjpKjxxPzQQw91eswaTR5Dhw714caVcNJJJwFp+TUpaZGmRZ9IksQ7g2pi1zZlqS3Crg76huIbQZIkRccbadEXb8vl83k/ueqw8UoWVuVumtpmbW9v9xO80maojmoDLfZCh361mdo0viloLLz++utFB37XKlGotqO1MA23oeM+Kzd3vP3220ChnnvuuWdVy9sbtM2tftB4EuF1U87Zu9YJXLuarzXnxG4IIbHrR6kE1brO1D56VOBNtdHva5Gz/fbb+2AULXgefvhhoNhhv5TLQGzs6FFuLYsWLfIuOHK/qJWjvm0FGoZhGIZhZIQpVoZRAXfffTdQcE6H8s6v+Xy+KlavVM2VK1fy6KOPVvz5qVOnAvDggw8CMGbMGCC14rUtrcfx48f717SFEh8XJJxzXvkq5/AuxS08MFWWqBTU559/Hki3Dp5++mmgsH0mS7Y3R5HEysxRRx3lX9O24NFHHw3AdtttB6Tb9Gqv8LgU1emxxx4DCgd8A0ybNg3o2kG6GtswpRSN733vewDsvvvuQKoI5HI5r25IeSuXXDI89Px3v/tdt3+71mh7ac6cOUCxYvXaa6/5/tMRSaKvjhrqihNPPBGAY445BiioLxr/UrOkxITb5pDWp7m52ddZR8EoEETb0tVGc47UoxUrVni1WMejZY2SQKvuujarjSlWhmEYhmEYGeFqaWE45/renDEMwzAMw+geTydJUlEeIlOsDMMwDMMwMsIWVoZhGIZhGBlhCyvDMAzDMIyMqHVU4BJgZcejURtGYO1da6zNa4u1d22x9q491ua1JWzvzSv9cE2d1wGcc09V6ghm9Bxr79pjbV5brL1ri7V37bE2ry29bW/bCjQMwzAMw8gIW1gZhmEYhmFkRF8srKb1wW9+lLH2rj3W5rXF2ru2WHvXHmvz2tKr9q65j5VhGIZhGMaHFdsKNAzDMAzDyIiaLaycc4c45+Y4515xzp1Xq9/9qOGcm+ece8E596xz7qmO54Y75+5xzr3c8Tisr8u5oeKcm+6cW+Scmxk8V7Z9nXPnd4z5Oc65g/um1Bs2Zdr8Yufc2x3j/Fnn3KHBa9bmvcA5t6lz7gHn3EvOuRedc2d3PG/jvAp00d42xquAc66fc+4/zrnnOtr7Rx3PZza+a7IV6JzLA3OBzwLzgRnA8UmSzKr6j3/EcM7NA3ZLkmRJ8NzPgGVJkkztWNQOS5Lk+31Vxg0Z59x+QBPwuyRJtu94rmT7Oue2A/4M7AGMBe4FPp4kSVsfFX+DpEybXww0JUny8+i91ua9xDk3BhiTJMkzzrnBwNPAkcBJ2DjPnC7a+xhsjGeOc84BA5MkaXLO1QOPAGcDXySj8V0rxWoP4JUkSV5LkqQFuBE4oka/bRTa+oaO/99A4aI1ekCSJA8By6Kny7XvEcCNSZKsSZLkdeAVCteCUQFl2rwc1ua9JEmSd5Ikeabj/yuAl4Bx2DivCl20dzmsvXtBUqCp48/6jn8JGY7vWi2sxgFvBX/Pp+uBY/ScBLjbOfe0c+7Ujuc2TpLkHShcxMCoPivdh5Ny7Wvjvrqc6Zx7vmOrULK9tXmGOOe2AHYBnsTGedWJ2htsjFcF51zeOfcssAi4J0mSTMd3rRZWrsRzFo5YHfZOkmQi8Hngmx3bKEbfYOO+evwaGA/sDLwDXNbxvLV5RjjnBgG3At9OkuSDrt5a4jlr8wop0d42xqtEkiRtSZLsDGwC7OGc276Lt1fc3rVaWM0HNg3+3gRYUKPf/kiRJMmCjsdFwF8pSJYLO/bxtZ+/qO9K+KGkXPvauK8SSZIs7Jgc24FrSKV5a/MM6PA9uRX4Y5Ikt3U8beO8SpRqbxvj1SdJkveAfwOHkOH4rtXCagawjXNuS+dcA3AccEeNfvsjg3NuYIfzI865gcDngJkU2npKx9umALf3TQk/tJRr3zuA45xzjc65LYFtgP/0Qfk+dGgC7OAoCuMcrM17TYdz73XAS0mS/CJ4ycZ5FSjX3jbGq4NzbqRzbmjH//sDBwGzyXB811Wj4DFJkrQ6584E/g/IA9OTJHmxFr/9EWNj4K+F65Q64E9JktzlnJsB3OSc+x/gTeDLfVjGDRrn3J+B/YERzrn5wA+BqZRo3yRJXnTO3QTMAlqBb1rkTuWUafP9nXM7U5Dk5wHfAGvzjNgbmAy80OGHAnABNs6rRbn2Pt7GeFUYA9zQka0gB9yUJMk/nHOPk9H4tszrhmEYhmEYGWGZ1w3DMAzDMDLCFlaGYRiGYRgZYQsrwzAMwzCMjLCFlWEYhmEYRkbYwsowDMMwDCMjbGFlGIZhGIaREbawMgzDMAzDyAhbWBmGYRiGYWTE/wOsbhKvlZh0UAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "np.set_printoptions(formatter=dict(int=lambda x: f'{x:4}'))\n",
    "\n",
    "# Grab the first batch of images\n",
    "for images,labels in train_loader: \n",
    "    break\n",
    "\n",
    "# Print the first 10 images\n",
    "im = make_grid(images[:10], nrow=10) \n",
    "plt.figure(figsize=(10,4))\n",
    "# We need to transpose the images from CWH to WHC\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));\n",
    "\n",
    "# Print the first 10 labels\n",
    "print('Labels: ', labels[:10].numpy())\n",
    "s = \"\"\n",
    "for label in labels[:10]:\n",
    "    s = s + \" \" + class_names[label.numpy()]\n",
    "print('Class: ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE\n",
    "# IMAGES ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE\n",
    "# IMAGES AND LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "<h3>3. If a 28x28 image is passed through a Convolutional layer using a 5x5 filter, a step size of 1, and no padding, what is the resulting matrix size?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px black solid; padding:5px'>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig size: torch.Size([10, 1, 28, 28])\n",
      "Down size: torch.Size([10, 1, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "###### ONLY RUN THIS TO CHECK YOUR ANSWER! ######\n",
    "################################################\n",
    "\n",
    "# Run the code below to check your answer:\n",
    "conv = nn.Conv2d(1, 1, 5, 1)\n",
    "for x,labels in train_loader:\n",
    "    print('Orig size:',x.shape)\n",
    "    break\n",
    "x = conv(x)\n",
    "print('Down size:',x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px black solid; padding:5px'>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down size: torch.Size([10, 1, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "###### ONLY RUN THIS TO CHECK YOUR ANSWER! ######\n",
    "################################################\n",
    "\n",
    "# Run the code below to check your answer:\n",
    "x = F.max_pool2d(x, 2, 2)\n",
    "print('Down size:',x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN definition\n",
    "### 5. Define a convolutional neural network\n",
    "Define a CNN model that can be trained on the Fashion-MNIST dataset. The model should contain two convolutional layers, two pooling layers, and two fully connected layers. You can use any number of neurons per layer so long as the model takes in a 28x28 image and returns an output of 10. Portions of the definition have been filled in for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        pass \n",
    "        return \n",
    "    \n",
    "torch.manual_seed(101)\n",
    "model = ConvolutionalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_sz=784, out_sz=10, layers=[120,84]):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_sz,layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0],layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1],out_sz)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptron(\n",
       "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "model = MultilayerPerceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is the total number of trainable parameters (weights & biases) in the model above?\n",
    "Answers will vary depending on your model definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px black solid; padding:5px'>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94080\n",
      "   120\n",
      " 10080\n",
      "    84\n",
      "   840\n",
      "    10\n",
      "______\n",
      "105214\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Define loss function & optimizer\n",
    "Define a loss function called \"criterion\" and an optimizer called \"optimizer\".<br>\n",
    "You can use any functions you want, although we used Cross Entropy Loss and Adam (learning rate of 0.001) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Load the first batch, print its shape\n",
    "for images, labels in train_loader:\n",
    "    print('Batch shape:', images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.view(10,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train the model\n",
    "Don't worry about tracking loss values, displaying results, or validating the test set. Just train the model through 5 epochs. We'll evaluate the trained model in the next step.<br>\n",
    "OPTIONAL: print something after each epoch to indicate training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch: 1000 [ 10000/60000]  loss: 0.54003364  accuracy:   7.235%\n",
      "epoch:  0  batch: 2000 [ 20000/60000]  loss: 0.24635518  accuracy:   7.737%\n",
      "epoch:  0  batch: 3000 [ 30000/60000]  loss: 0.86251193  accuracy:   7.954%\n",
      "epoch:  0  batch: 4000 [ 40000/60000]  loss: 0.32126194  accuracy:   8.073%\n",
      "epoch:  0  batch: 5000 [ 50000/60000]  loss: 0.24082506  accuracy:   8.148%\n",
      "epoch:  0  batch: 6000 [ 60000/60000]  loss: 0.34826809  accuracy:   8.213%\n",
      " 0 of 15 epochs completed\n",
      "epoch:  1  batch: 1000 [ 10000/60000]  loss: 0.27878314  accuracy:   8.643%\n",
      "epoch:  1  batch: 2000 [ 20000/60000]  loss: 0.18444511  accuracy:   8.603%\n",
      "epoch:  1  batch: 3000 [ 30000/60000]  loss: 0.25811249  accuracy:   8.619%\n",
      "epoch:  1  batch: 4000 [ 40000/60000]  loss: 0.48130912  accuracy:   8.637%\n",
      "epoch:  1  batch: 5000 [ 50000/60000]  loss: 0.10155133  accuracy:   8.640%\n",
      "epoch:  1  batch: 6000 [ 60000/60000]  loss: 0.17266577  accuracy:   8.651%\n",
      " 1 of 15 epochs completed\n",
      "epoch:  2  batch: 1000 [ 10000/60000]  loss: 0.18292828  accuracy:   8.725%\n",
      "epoch:  2  batch: 2000 [ 20000/60000]  loss: 1.12070119  accuracy:   8.765%\n",
      "epoch:  2  batch: 3000 [ 30000/60000]  loss: 0.14978687  accuracy:   8.756%\n",
      "epoch:  2  batch: 4000 [ 40000/60000]  loss: 0.23819578  accuracy:   8.773%\n",
      "epoch:  2  batch: 5000 [ 50000/60000]  loss: 0.07836673  accuracy:   8.774%\n",
      "epoch:  2  batch: 6000 [ 60000/60000]  loss: 0.08873417  accuracy:   8.780%\n",
      " 2 of 15 epochs completed\n",
      "epoch:  3  batch: 1000 [ 10000/60000]  loss: 0.34472308  accuracy:   8.855%\n",
      "epoch:  3  batch: 2000 [ 20000/60000]  loss: 0.87660092  accuracy:   8.851%\n",
      "epoch:  3  batch: 3000 [ 30000/60000]  loss: 0.28324959  accuracy:   8.846%\n",
      "epoch:  3  batch: 4000 [ 40000/60000]  loss: 0.15028800  accuracy:   8.836%\n",
      "epoch:  3  batch: 5000 [ 50000/60000]  loss: 0.31726667  accuracy:   8.843%\n",
      "epoch:  3  batch: 6000 [ 60000/60000]  loss: 0.53741539  accuracy:   8.842%\n",
      " 3 of 15 epochs completed\n",
      "epoch:  4  batch: 1000 [ 10000/60000]  loss: 0.26416534  accuracy:   8.924%\n",
      "epoch:  4  batch: 2000 [ 20000/60000]  loss: 0.12637536  accuracy:   8.916%\n",
      "epoch:  4  batch: 3000 [ 30000/60000]  loss: 0.68204153  accuracy:   8.918%\n",
      "epoch:  4  batch: 4000 [ 40000/60000]  loss: 0.03794850  accuracy:   8.910%\n",
      "epoch:  4  batch: 5000 [ 50000/60000]  loss: 0.37444553  accuracy:   8.913%\n",
      "epoch:  4  batch: 6000 [ 60000/60000]  loss: 0.51129472  accuracy:   8.911%\n",
      " 4 of 15 epochs completed\n",
      "epoch:  5  batch: 1000 [ 10000/60000]  loss: 0.19888642  accuracy:   8.912%\n",
      "epoch:  5  batch: 2000 [ 20000/60000]  loss: 0.07898112  accuracy:   8.941%\n",
      "epoch:  5  batch: 3000 [ 30000/60000]  loss: 0.01942935  accuracy:   8.947%\n",
      "epoch:  5  batch: 4000 [ 40000/60000]  loss: 0.85125351  accuracy:   8.950%\n",
      "epoch:  5  batch: 5000 [ 50000/60000]  loss: 0.39688259  accuracy:   8.940%\n",
      "epoch:  5  batch: 6000 [ 60000/60000]  loss: 0.42072290  accuracy:   8.933%\n",
      " 5 of 15 epochs completed\n",
      "epoch:  6  batch: 1000 [ 10000/60000]  loss: 0.45821142  accuracy:   9.001%\n",
      "epoch:  6  batch: 2000 [ 20000/60000]  loss: 0.25909334  accuracy:   8.996%\n",
      "epoch:  6  batch: 3000 [ 30000/60000]  loss: 0.20704198  accuracy:   8.988%\n",
      "epoch:  6  batch: 4000 [ 40000/60000]  loss: 0.22254284  accuracy:   8.993%\n",
      "epoch:  6  batch: 5000 [ 50000/60000]  loss: 0.09175877  accuracy:   8.991%\n",
      "epoch:  6  batch: 6000 [ 60000/60000]  loss: 0.13886833  accuracy:   8.983%\n",
      " 6 of 15 epochs completed\n",
      "epoch:  7  batch: 1000 [ 10000/60000]  loss: 0.09691332  accuracy:   9.053%\n",
      "epoch:  7  batch: 2000 [ 20000/60000]  loss: 0.03819124  accuracy:   9.038%\n",
      "epoch:  7  batch: 3000 [ 30000/60000]  loss: 0.33468971  accuracy:   9.026%\n",
      "epoch:  7  batch: 4000 [ 40000/60000]  loss: 1.04117012  accuracy:   9.026%\n",
      "epoch:  7  batch: 5000 [ 50000/60000]  loss: 0.42285505  accuracy:   9.025%\n",
      "epoch:  7  batch: 6000 [ 60000/60000]  loss: 0.57964790  accuracy:   9.012%\n",
      " 7 of 15 epochs completed\n",
      "epoch:  8  batch: 1000 [ 10000/60000]  loss: 0.06414311  accuracy:   9.092%\n",
      "epoch:  8  batch: 2000 [ 20000/60000]  loss: 0.08661262  accuracy:   9.055%\n",
      "epoch:  8  batch: 3000 [ 30000/60000]  loss: 0.12567498  accuracy:   9.068%\n",
      "epoch:  8  batch: 4000 [ 40000/60000]  loss: 0.02165763  accuracy:   9.058%\n",
      "epoch:  8  batch: 5000 [ 50000/60000]  loss: 0.25289199  accuracy:   9.054%\n",
      "epoch:  8  batch: 6000 [ 60000/60000]  loss: 0.02248537  accuracy:   9.047%\n",
      " 8 of 15 epochs completed\n",
      "epoch:  9  batch: 1000 [ 10000/60000]  loss: 0.38296825  accuracy:   9.044%\n",
      "epoch:  9  batch: 2000 [ 20000/60000]  loss: 0.43304387  accuracy:   9.053%\n",
      "epoch:  9  batch: 3000 [ 30000/60000]  loss: 0.30456886  accuracy:   9.055%\n",
      "epoch:  9  batch: 4000 [ 40000/60000]  loss: 0.88125545  accuracy:   9.065%\n",
      "epoch:  9  batch: 5000 [ 50000/60000]  loss: 0.30909306  accuracy:   9.066%\n",
      "epoch:  9  batch: 6000 [ 60000/60000]  loss: 0.00580816  accuracy:   9.067%\n",
      " 9 of 15 epochs completed\n",
      "epoch: 10  batch: 1000 [ 10000/60000]  loss: 0.11438131  accuracy:   9.112%\n",
      "epoch: 10  batch: 2000 [ 20000/60000]  loss: 0.00881433  accuracy:   9.120%\n",
      "epoch: 10  batch: 3000 [ 30000/60000]  loss: 0.23161633  accuracy:   9.109%\n",
      "epoch: 10  batch: 4000 [ 40000/60000]  loss: 0.33743998  accuracy:   9.103%\n",
      "epoch: 10  batch: 5000 [ 50000/60000]  loss: 0.05183147  accuracy:   9.095%\n",
      "epoch: 10  batch: 6000 [ 60000/60000]  loss: 0.01671260  accuracy:   9.095%\n",
      "10 of 15 epochs completed\n",
      "epoch: 11  batch: 1000 [ 10000/60000]  loss: 0.01717766  accuracy:   9.136%\n",
      "epoch: 11  batch: 2000 [ 20000/60000]  loss: 0.35162851  accuracy:   9.116%\n",
      "epoch: 11  batch: 3000 [ 30000/60000]  loss: 0.15037718  accuracy:   9.123%\n",
      "epoch: 11  batch: 4000 [ 40000/60000]  loss: 0.10746938  accuracy:   9.117%\n",
      "epoch: 11  batch: 5000 [ 50000/60000]  loss: 0.09624408  accuracy:   9.104%\n",
      "epoch: 11  batch: 6000 [ 60000/60000]  loss: 0.64947999  accuracy:   9.115%\n",
      "11 of 15 epochs completed\n",
      "epoch: 12  batch: 1000 [ 10000/60000]  loss: 0.49327278  accuracy:   9.151%\n",
      "epoch: 12  batch: 2000 [ 20000/60000]  loss: 0.27223748  accuracy:   9.170%\n",
      "epoch: 12  batch: 3000 [ 30000/60000]  loss: 0.11860204  accuracy:   9.148%\n",
      "epoch: 12  batch: 4000 [ 40000/60000]  loss: 0.22784647  accuracy:   9.153%\n",
      "epoch: 12  batch: 5000 [ 50000/60000]  loss: 0.37345931  accuracy:   9.137%\n",
      "epoch: 12  batch: 6000 [ 60000/60000]  loss: 0.19747920  accuracy:   9.141%\n",
      "12 of 15 epochs completed\n",
      "epoch: 13  batch: 1000 [ 10000/60000]  loss: 0.43761483  accuracy:   9.200%\n",
      "epoch: 13  batch: 2000 [ 20000/60000]  loss: 0.03984049  accuracy:   9.169%\n",
      "epoch: 13  batch: 3000 [ 30000/60000]  loss: 0.06170886  accuracy:   9.170%\n",
      "epoch: 13  batch: 4000 [ 40000/60000]  loss: 0.49974909  accuracy:   9.165%\n",
      "epoch: 13  batch: 5000 [ 50000/60000]  loss: 0.04219783  accuracy:   9.160%\n",
      "epoch: 13  batch: 6000 [ 60000/60000]  loss: 0.07986536  accuracy:   9.159%\n",
      "13 of 15 epochs completed\n",
      "epoch: 14  batch: 1000 [ 10000/60000]  loss: 0.50074965  accuracy:   9.173%\n",
      "epoch: 14  batch: 2000 [ 20000/60000]  loss: 1.39106047  accuracy:   9.172%\n",
      "epoch: 14  batch: 3000 [ 30000/60000]  loss: 0.57615042  accuracy:   9.178%\n",
      "epoch: 14  batch: 4000 [ 40000/60000]  loss: 0.05431701  accuracy:   9.182%\n",
      "epoch: 14  batch: 5000 [ 50000/60000]  loss: 0.19608939  accuracy:   9.173%\n",
      "epoch: 14  batch: 6000 [ 60000/60000]  loss: 0.32137528  accuracy:   9.169%\n",
      "14 of 15 epochs completed\n",
      "\n",
      "Duration: 250 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 15\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train.view(10, -1))  # Here we flatten X_train\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print interim results\n",
    "        if b%1000 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*10/(10*b):7.3f}%')\n",
    "\n",
    "    print(f\"{i:2} of {epochs:2} epochs completed\")\n",
    "    # Update train loss & accuracy for the epoch\n",
    "    \n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = model(X_test.view(10, -1))  # Here we flatten X_test\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "    \n",
    "    # Update test loss & accuracy for the epoch\n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluate the model\n",
    "Set <tt>model.eval()</tt> and determine the percentage correct out of 10,000 total test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(8157), tensor(8505), tensor(8640), tensor(8766), tensor(8627)]\n",
      "\n",
      "Test accuracy: 8.627%\n"
     ]
    }
   ],
   "source": [
    "print(test_correct) # contains the results of all 5 epochs\n",
    "print()\n",
    "print(f'Test accuracy: {test_correct[-1].item()*10/10000:.3f}%') # print the most recent result as a percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
