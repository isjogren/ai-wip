{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Exercises\n",
    "For these exercises we'll work with the <a href='https://www.kaggle.com/zalando-research/fashionmnist'>Fashion-MNIST</a> dataset, also available through <a href='https://pytorch.org/docs/stable/torchvision/index.html'><tt><strong>torchvision</strong></tt></a>. Like MNIST, this dataset consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes:\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "<div class=\"alert alert-danger\" style=\"margin: 10px\"><strong>IMPORTANT NOTE!</strong> Make sure you don't run the cells directly above the example output shown, <br>otherwise you will end up writing over the example output!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform standard imports, load the Fashion-MNIST dataset\n",
    "Run the cell below to load the libraries needed for this exercise and the Fashion-MNIST dataset.<br>\n",
    "PyTorch makes the Fashion-MNIST dataset available through <a href='https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist'><tt><strong>torchvision</strong></tt></a>. The first time it's called, the dataset will be downloaded onto your computer to the path specified. From that point, torchvision will always look for a local copy before attempting another download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sschoen/.conda/envs/pytorchenv/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='../Data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='../Data', train=False, download=True, transform=transform)\n",
    "\n",
    "class_names = ['T-shirt','Trouser','Sweater','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create data loaders\n",
    "Use DataLoader to create a <tt>train_loader</tt> and a <tt>test_loader</tt>. Batch sizes should be 10 for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ../Data\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: ../Data\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine a batch of images\n",
    "Use DataLoader, <tt>make_grid</tt> and matplotlib to display the first batch of 10 images.<br>\n",
    "OPTIONAL: display the labels as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  [   0    8    0    3    8    7    2    5    2    0]\n",
      "Class:   T-shirt Bag T-shirt Dress Bag Sneaker Sweater Sandal Sweater T-shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "np.set_printoptions(formatter=dict(int=lambda x: f'{x:4}'))\n",
    "\n",
    "# Grab the first batch of images\n",
    "for images,labels in train_loader: \n",
    "    break\n",
    "\n",
    "# Print the first 10 images\n",
    "im = make_grid(images[:10], nrow=10) \n",
    "plt.figure(figsize=(10,4))\n",
    "# We need to transpose the images from CWH to WHC\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));\n",
    "\n",
    "# Print the first 10 labels\n",
    "print('Labels: ', labels[:10].numpy())\n",
    "s = \"\"\n",
    "for label in labels[:10]:\n",
    "    s = s + \" \" + class_names[label.numpy()]\n",
    "print('Class: ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE\n",
    "# IMAGES ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE\n",
    "# IMAGES AND LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "<h3>3. If a 28x28 image is passed through a Convolutional layer using a 5x5 filter, a step size of 1, and no padding, what is the resulting matrix size?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px black solid; padding:5px'>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "###### ONLY RUN THIS TO CHECK YOUR ANSWER! ######\n",
    "################################################\n",
    "\n",
    "# Run the code below to check your answer:\n",
    "conv = nn.Conv2d(1, 1, 5, 1)\n",
    "for x,labels in train_loader:\n",
    "    print('Orig size:',x.shape)\n",
    "    break\n",
    "x = conv(x)\n",
    "print('Down size:',x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. If the sample from question 3 is then passed through a 2x2 MaxPooling layer, what is the resulting matrix size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px black solid; padding:5px'>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "###### ONLY RUN THIS TO CHECK YOUR ANSWER! ######\n",
    "################################################\n",
    "\n",
    "# Run the code below to check your answer:\n",
    "x = F.max_pool2d(x, 2, 2)\n",
    "print('Down size:',x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN definition\n",
    "### 5. Define a convolutional neural network\n",
    "Define a CNN model that can be trained on the Fashion-MNIST dataset. The model should contain two convolutional layers, two pooling layers, and two fully connected layers. You can use any number of neurons per layer so long as the model takes in a 28x28 image and returns an output of 10. Portions of the definition have been filled in for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        pass \n",
    "        return \n",
    "    \n",
    "torch.manual_seed(101)\n",
    "model = ConvolutionalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_sz=784, out_sz=10, layers=[120,84]):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_sz,layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0],layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1],out_sz)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptron(\n",
       "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "model = MultilayerPerceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is the total number of trainable parameters (weights & biases) in the model above?\n",
    "Answers will vary depending on your model definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border:1px black solid; padding:5px'>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94080\n",
      "   120\n",
      " 10080\n",
      "    84\n",
      "   840\n",
      "    10\n",
      "______\n",
      "105214\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Define loss function & optimizer\n",
    "Define a loss function called \"criterion\" and an optimizer called \"optimizer\".<br>\n",
    "You can use any functions you want, although we used Cross Entropy Loss and Adam (learning rate of 0.001) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T WRITE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Load the first batch, print its shape\n",
    "for images, labels in train_loader:\n",
    "    print('Batch shape:', images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.view(10,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train the model\n",
    "Don't worry about tracking loss values, displaying results, or validating the test set. Just train the model through 5 epochs. We'll evaluate the trained model in the next step.<br>\n",
    "OPTIONAL: print something after each epoch to indicate training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:  200 [  2000/60000]  loss: 1.04767740  accuracy:   5.815%\n",
      "epoch:  0  batch:  400 [  4000/60000]  loss: 1.06330240  accuracy:   6.665%\n",
      "epoch:  0  batch:  600 [  6000/60000]  loss: 0.96007001  accuracy:   7.003%\n",
      "epoch:  0  batch:  800 [  8000/60000]  loss: 0.61146700  accuracy:   7.234%\n",
      "epoch:  0  batch: 1000 [ 10000/60000]  loss: 0.58113408  accuracy:   7.341%\n",
      "epoch:  0  batch: 1200 [ 12000/60000]  loss: 0.81224668  accuracy:   7.466%\n",
      "epoch:  0  batch: 1400 [ 14000/60000]  loss: 0.62057287  accuracy:   7.581%\n",
      "epoch:  0  batch: 1600 [ 16000/60000]  loss: 0.35336220  accuracy:   7.656%\n",
      "epoch:  0  batch: 1800 [ 18000/60000]  loss: 0.36065903  accuracy:   7.733%\n",
      "epoch:  0  batch: 2000 [ 20000/60000]  loss: 0.33764386  accuracy:   7.793%\n",
      "epoch:  0  batch: 2200 [ 22000/60000]  loss: 0.42147088  accuracy:   7.837%\n",
      "epoch:  0  batch: 2400 [ 24000/60000]  loss: 0.37662899  accuracy:   7.881%\n",
      "epoch:  0  batch: 2600 [ 26000/60000]  loss: 0.48119664  accuracy:   7.901%\n",
      "epoch:  0  batch: 2800 [ 28000/60000]  loss: 1.01233196  accuracy:   7.935%\n",
      "epoch:  0  batch: 3000 [ 30000/60000]  loss: 0.65824813  accuracy:   7.964%\n",
      "epoch:  0  batch: 3200 [ 32000/60000]  loss: 0.58663201  accuracy:   7.990%\n",
      "epoch:  0  batch: 3400 [ 34000/60000]  loss: 0.25524765  accuracy:   8.020%\n",
      "epoch:  0  batch: 3600 [ 36000/60000]  loss: 0.82568491  accuracy:   8.046%\n",
      "epoch:  0  batch: 3800 [ 38000/60000]  loss: 0.37203044  accuracy:   8.066%\n",
      "epoch:  0  batch: 4000 [ 40000/60000]  loss: 0.64321285  accuracy:   8.080%\n",
      "epoch:  0  batch: 4200 [ 42000/60000]  loss: 0.70297730  accuracy:   8.093%\n",
      "epoch:  0  batch: 4400 [ 44000/60000]  loss: 0.41755018  accuracy:   8.115%\n",
      "epoch:  0  batch: 4600 [ 46000/60000]  loss: 0.09766512  accuracy:   8.130%\n",
      "epoch:  0  batch: 4800 [ 48000/60000]  loss: 0.09715097  accuracy:   8.146%\n",
      "epoch:  0  batch: 5000 [ 50000/60000]  loss: 0.76525271  accuracy:   8.163%\n",
      "epoch:  0  batch: 5200 [ 52000/60000]  loss: 0.29661638  accuracy:   8.178%\n",
      "epoch:  0  batch: 5400 [ 54000/60000]  loss: 0.88884830  accuracy:   8.187%\n",
      "epoch:  0  batch: 5600 [ 56000/60000]  loss: 0.59073627  accuracy:   8.199%\n",
      "epoch:  0  batch: 5800 [ 58000/60000]  loss: 0.20927259  accuracy:   8.209%\n",
      "epoch:  0  batch: 6000 [ 60000/60000]  loss: 0.09036511  accuracy:   8.222%\n",
      " 0 of  5 epochs completed\n",
      "epoch:  1  batch:  200 [  2000/60000]  loss: 0.09211004  accuracy:   8.630%\n",
      "epoch:  1  batch:  400 [  4000/60000]  loss: 0.13075256  accuracy:   8.623%\n",
      "epoch:  1  batch:  600 [  6000/60000]  loss: 0.26932234  accuracy:   8.613%\n",
      "epoch:  1  batch:  800 [  8000/60000]  loss: 0.31586468  accuracy:   8.627%\n",
      "epoch:  1  batch: 1000 [ 10000/60000]  loss: 0.38663751  accuracy:   8.618%\n",
      "epoch:  1  batch: 1200 [ 12000/60000]  loss: 0.53670484  accuracy:   8.632%\n",
      "epoch:  1  batch: 1400 [ 14000/60000]  loss: 0.66583908  accuracy:   8.612%\n",
      "epoch:  1  batch: 1600 [ 16000/60000]  loss: 0.29120058  accuracy:   8.614%\n",
      "epoch:  1  batch: 1800 [ 18000/60000]  loss: 0.46740398  accuracy:   8.608%\n",
      "epoch:  1  batch: 2000 [ 20000/60000]  loss: 0.56465614  accuracy:   8.618%\n",
      "epoch:  1  batch: 2200 [ 22000/60000]  loss: 0.22633004  accuracy:   8.620%\n",
      "epoch:  1  batch: 2400 [ 24000/60000]  loss: 0.91917199  accuracy:   8.620%\n",
      "epoch:  1  batch: 2600 [ 26000/60000]  loss: 0.46039501  accuracy:   8.622%\n",
      "epoch:  1  batch: 2800 [ 28000/60000]  loss: 0.42720509  accuracy:   8.632%\n",
      "epoch:  1  batch: 3000 [ 30000/60000]  loss: 0.19770037  accuracy:   8.630%\n",
      "epoch:  1  batch: 3200 [ 32000/60000]  loss: 0.04808810  accuracy:   8.637%\n",
      "epoch:  1  batch: 3400 [ 34000/60000]  loss: 0.34470299  accuracy:   8.640%\n",
      "epoch:  1  batch: 3600 [ 36000/60000]  loss: 1.03743947  accuracy:   8.641%\n",
      "epoch:  1  batch: 3800 [ 38000/60000]  loss: 0.10858363  accuracy:   8.641%\n",
      "epoch:  1  batch: 4000 [ 40000/60000]  loss: 0.19450541  accuracy:   8.645%\n",
      "epoch:  1  batch: 4200 [ 42000/60000]  loss: 0.51020992  accuracy:   8.647%\n",
      "epoch:  1  batch: 4400 [ 44000/60000]  loss: 0.27721363  accuracy:   8.652%\n",
      "epoch:  1  batch: 4600 [ 46000/60000]  loss: 0.09155698  accuracy:   8.650%\n",
      "epoch:  1  batch: 4800 [ 48000/60000]  loss: 0.20441766  accuracy:   8.652%\n",
      "epoch:  1  batch: 5000 [ 50000/60000]  loss: 0.10469959  accuracy:   8.654%\n",
      "epoch:  1  batch: 5200 [ 52000/60000]  loss: 0.21160249  accuracy:   8.662%\n",
      "epoch:  1  batch: 5400 [ 54000/60000]  loss: 0.67608118  accuracy:   8.661%\n",
      "epoch:  1  batch: 5600 [ 56000/60000]  loss: 0.19323231  accuracy:   8.658%\n",
      "epoch:  1  batch: 5800 [ 58000/60000]  loss: 0.28849179  accuracy:   8.653%\n",
      "epoch:  1  batch: 6000 [ 60000/60000]  loss: 0.30582845  accuracy:   8.653%\n",
      " 1 of  5 epochs completed\n",
      "epoch:  2  batch:  200 [  2000/60000]  loss: 0.22422266  accuracy:   8.800%\n",
      "epoch:  2  batch:  400 [  4000/60000]  loss: 0.80502862  accuracy:   8.688%\n",
      "epoch:  2  batch:  600 [  6000/60000]  loss: 0.30431768  accuracy:   8.713%\n",
      "epoch:  2  batch:  800 [  8000/60000]  loss: 0.33428955  accuracy:   8.707%\n",
      "epoch:  2  batch: 1000 [ 10000/60000]  loss: 1.27510941  accuracy:   8.739%\n",
      "epoch:  2  batch: 1200 [ 12000/60000]  loss: 0.47557205  accuracy:   8.729%\n",
      "epoch:  2  batch: 1400 [ 14000/60000]  loss: 0.51290661  accuracy:   8.721%\n",
      "epoch:  2  batch: 1600 [ 16000/60000]  loss: 0.26001939  accuracy:   8.711%\n",
      "epoch:  2  batch: 1800 [ 18000/60000]  loss: 0.46315518  accuracy:   8.716%\n",
      "epoch:  2  batch: 2000 [ 20000/60000]  loss: 0.21229501  accuracy:   8.724%\n",
      "epoch:  2  batch: 2200 [ 22000/60000]  loss: 0.57871795  accuracy:   8.720%\n",
      "epoch:  2  batch: 2400 [ 24000/60000]  loss: 0.11731055  accuracy:   8.733%\n",
      "epoch:  2  batch: 2600 [ 26000/60000]  loss: 0.10173841  accuracy:   8.737%\n",
      "epoch:  2  batch: 2800 [ 28000/60000]  loss: 0.15201576  accuracy:   8.740%\n",
      "epoch:  2  batch: 3000 [ 30000/60000]  loss: 0.82994223  accuracy:   8.746%\n",
      "epoch:  2  batch: 3200 [ 32000/60000]  loss: 0.25849336  accuracy:   8.749%\n",
      "epoch:  2  batch: 3400 [ 34000/60000]  loss: 0.35361832  accuracy:   8.743%\n",
      "epoch:  2  batch: 3600 [ 36000/60000]  loss: 0.08278114  accuracy:   8.744%\n",
      "epoch:  2  batch: 3800 [ 38000/60000]  loss: 0.09746700  accuracy:   8.748%\n",
      "epoch:  2  batch: 4000 [ 40000/60000]  loss: 0.48478207  accuracy:   8.752%\n",
      "epoch:  2  batch: 4200 [ 42000/60000]  loss: 0.10066335  accuracy:   8.758%\n",
      "epoch:  2  batch: 4400 [ 44000/60000]  loss: 0.05084682  accuracy:   8.757%\n",
      "epoch:  2  batch: 4600 [ 46000/60000]  loss: 0.29956239  accuracy:   8.759%\n",
      "epoch:  2  batch: 4800 [ 48000/60000]  loss: 0.07042307  accuracy:   8.763%\n",
      "epoch:  2  batch: 5000 [ 50000/60000]  loss: 0.08859492  accuracy:   8.765%\n",
      "epoch:  2  batch: 5200 [ 52000/60000]  loss: 0.02281610  accuracy:   8.763%\n",
      "epoch:  2  batch: 5400 [ 54000/60000]  loss: 0.18827261  accuracy:   8.764%\n",
      "epoch:  2  batch: 5600 [ 56000/60000]  loss: 0.83391654  accuracy:   8.764%\n",
      "epoch:  2  batch: 5800 [ 58000/60000]  loss: 0.19856274  accuracy:   8.765%\n",
      "epoch:  2  batch: 6000 [ 60000/60000]  loss: 0.19423550  accuracy:   8.768%\n",
      " 2 of  5 epochs completed\n",
      "epoch:  3  batch:  200 [  2000/60000]  loss: 0.28574774  accuracy:   8.920%\n",
      "epoch:  3  batch:  400 [  4000/60000]  loss: 0.30311650  accuracy:   8.875%\n",
      "epoch:  3  batch:  600 [  6000/60000]  loss: 0.18747810  accuracy:   8.918%\n",
      "epoch:  3  batch:  800 [  8000/60000]  loss: 0.56394392  accuracy:   8.855%\n",
      "epoch:  3  batch: 1000 [ 10000/60000]  loss: 0.09074147  accuracy:   8.849%\n",
      "epoch:  3  batch: 1200 [ 12000/60000]  loss: 0.44081068  accuracy:   8.847%\n",
      "epoch:  3  batch: 1400 [ 14000/60000]  loss: 0.65125734  accuracy:   8.839%\n",
      "epoch:  3  batch: 1600 [ 16000/60000]  loss: 0.36188486  accuracy:   8.839%\n",
      "epoch:  3  batch: 1800 [ 18000/60000]  loss: 0.30884346  accuracy:   8.856%\n",
      "epoch:  3  batch: 2000 [ 20000/60000]  loss: 0.20799974  accuracy:   8.848%\n",
      "epoch:  3  batch: 2200 [ 22000/60000]  loss: 0.08515239  accuracy:   8.850%\n",
      "epoch:  3  batch: 2400 [ 24000/60000]  loss: 0.10407759  accuracy:   8.855%\n",
      "epoch:  3  batch: 2600 [ 26000/60000]  loss: 0.54590803  accuracy:   8.853%\n",
      "epoch:  3  batch: 2800 [ 28000/60000]  loss: 0.00649927  accuracy:   8.855%\n",
      "epoch:  3  batch: 3000 [ 30000/60000]  loss: 0.46248537  accuracy:   8.853%\n",
      "epoch:  3  batch: 3200 [ 32000/60000]  loss: 0.22684768  accuracy:   8.852%\n",
      "epoch:  3  batch: 3400 [ 34000/60000]  loss: 0.59574729  accuracy:   8.854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3  batch: 3600 [ 36000/60000]  loss: 0.19762787  accuracy:   8.855%\n",
      "epoch:  3  batch: 3800 [ 38000/60000]  loss: 0.04970515  accuracy:   8.852%\n",
      "epoch:  3  batch: 4000 [ 40000/60000]  loss: 0.26623613  accuracy:   8.851%\n",
      "epoch:  3  batch: 4200 [ 42000/60000]  loss: 0.50658274  accuracy:   8.852%\n",
      "epoch:  3  batch: 4400 [ 44000/60000]  loss: 0.08573924  accuracy:   8.854%\n",
      "epoch:  3  batch: 4600 [ 46000/60000]  loss: 0.26767951  accuracy:   8.848%\n",
      "epoch:  3  batch: 4800 [ 48000/60000]  loss: 0.46702567  accuracy:   8.848%\n",
      "epoch:  3  batch: 5000 [ 50000/60000]  loss: 0.48256677  accuracy:   8.848%\n",
      "epoch:  3  batch: 5200 [ 52000/60000]  loss: 0.54611045  accuracy:   8.847%\n",
      "epoch:  3  batch: 5400 [ 54000/60000]  loss: 0.40678796  accuracy:   8.838%\n",
      "epoch:  3  batch: 5600 [ 56000/60000]  loss: 0.34125376  accuracy:   8.838%\n",
      "epoch:  3  batch: 5800 [ 58000/60000]  loss: 0.12204103  accuracy:   8.840%\n",
      "epoch:  3  batch: 6000 [ 60000/60000]  loss: 0.13320914  accuracy:   8.841%\n",
      " 3 of  5 epochs completed\n",
      "epoch:  4  batch:  200 [  2000/60000]  loss: 0.51941180  accuracy:   8.975%\n",
      "epoch:  4  batch:  400 [  4000/60000]  loss: 0.10922394  accuracy:   8.988%\n",
      "epoch:  4  batch:  600 [  6000/60000]  loss: 0.21690588  accuracy:   8.960%\n",
      "epoch:  4  batch:  800 [  8000/60000]  loss: 0.37301022  accuracy:   8.940%\n",
      "epoch:  4  batch: 1000 [ 10000/60000]  loss: 0.59755456  accuracy:   8.939%\n",
      "epoch:  4  batch: 1200 [ 12000/60000]  loss: 0.06976413  accuracy:   8.922%\n",
      "epoch:  4  batch: 1400 [ 14000/60000]  loss: 0.04423757  accuracy:   8.931%\n",
      "epoch:  4  batch: 1600 [ 16000/60000]  loss: 0.13975795  accuracy:   8.924%\n",
      "epoch:  4  batch: 1800 [ 18000/60000]  loss: 0.23622198  accuracy:   8.931%\n",
      "epoch:  4  batch: 2000 [ 20000/60000]  loss: 0.03924950  accuracy:   8.932%\n",
      "epoch:  4  batch: 2200 [ 22000/60000]  loss: 0.58751118  accuracy:   8.929%\n",
      "epoch:  4  batch: 2400 [ 24000/60000]  loss: 0.14737606  accuracy:   8.932%\n",
      "epoch:  4  batch: 2600 [ 26000/60000]  loss: 0.39840761  accuracy:   8.927%\n",
      "epoch:  4  batch: 2800 [ 28000/60000]  loss: 0.25738934  accuracy:   8.929%\n",
      "epoch:  4  batch: 3000 [ 30000/60000]  loss: 0.50299358  accuracy:   8.927%\n",
      "epoch:  4  batch: 3200 [ 32000/60000]  loss: 0.08854763  accuracy:   8.926%\n",
      "epoch:  4  batch: 3400 [ 34000/60000]  loss: 0.38612598  accuracy:   8.922%\n",
      "epoch:  4  batch: 3600 [ 36000/60000]  loss: 0.46789914  accuracy:   8.922%\n",
      "epoch:  4  batch: 3800 [ 38000/60000]  loss: 0.29972225  accuracy:   8.923%\n",
      "epoch:  4  batch: 4000 [ 40000/60000]  loss: 0.51569939  accuracy:   8.912%\n",
      "epoch:  4  batch: 4200 [ 42000/60000]  loss: 0.02837365  accuracy:   8.911%\n",
      "epoch:  4  batch: 4400 [ 44000/60000]  loss: 0.08690239  accuracy:   8.905%\n",
      "epoch:  4  batch: 4600 [ 46000/60000]  loss: 0.28924900  accuracy:   8.906%\n",
      "epoch:  4  batch: 4800 [ 48000/60000]  loss: 0.00747109  accuracy:   8.906%\n",
      "epoch:  4  batch: 5000 [ 50000/60000]  loss: 0.44242257  accuracy:   8.905%\n",
      "epoch:  4  batch: 5200 [ 52000/60000]  loss: 0.37823567  accuracy:   8.906%\n",
      "epoch:  4  batch: 5400 [ 54000/60000]  loss: 0.42947736  accuracy:   8.901%\n",
      "epoch:  4  batch: 5600 [ 56000/60000]  loss: 0.02949519  accuracy:   8.901%\n",
      "epoch:  4  batch: 5800 [ 58000/60000]  loss: 0.07219382  accuracy:   8.902%\n",
      "epoch:  4  batch: 6000 [ 60000/60000]  loss: 0.25342354  accuracy:   8.905%\n",
      " 4 of  5 epochs completed\n",
      "\n",
      "Duration: 147 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train.view(10, -1))  # Here we flatten X_train\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print interim results\n",
    "        if b%1000 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*10/(10*b):7.3f}%')\n",
    "\n",
    "    print(f\"{i:2} of {epochs:2} epochs completed\")\n",
    "    # Update train loss & accuracy for the epoch\n",
    "    \n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = model(X_test.view(10, -1))  # Here we flatten X_test\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "    \n",
    "    # Update test loss & accuracy for the epoch\n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluate the model\n",
    "Set <tt>model.eval()</tt> and determine the percentage correct out of 10,000 total test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(8157), tensor(8505), tensor(8640), tensor(8766), tensor(8627)]\n",
      "\n",
      "Test accuracy: 8.627%\n"
     ]
    }
   ],
   "source": [
    "print(test_correct) # contains the results of all 5 epochs\n",
    "print()\n",
    "print(f'Test accuracy: {test_correct[-1].item()*10/10000:.3f}%') # print the most recent result as a percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
